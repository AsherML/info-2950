{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python383jvsc74a57bd0f62ec7ccb9190cdf7c696339bf5a0173a2767af81ddd8dbdd8c4c12684be5b27",
   "display_name": "Python 3.8.3 64-bit ('info3350': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "f62ec7ccb9190cdf7c696339bf5a0173a2767af81ddd8dbdd8c4c12684be5b27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Data Description"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Why was this dataset created/how was it funded?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Our initial data came from a corpus compiled by researchers at UC Santa Cruz center for Natural Language and Dialogue Systems entitled (appropriately enough) Film Corpus 2.0. Here’s the full citation: (Marilyn A. Walker, Grace I. Lin, Jennifer E. Sawyer. \"An Annotated Corpus of Film Dialogue for Learning and Characterizing Character Style.\" In Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC), Istanbul, Turkey, 2012). The corpus was originally designed to serve as a general purpose dataset for researchers interested in natural language problems involving dialogue and conversation. This corpus consisted of 862 film scripts downloaded from the internet movie scripts database (imsdb.com) separated by the genre they were labeled as. Ostensibly, there was minimum cleaning performed on the datasets to isolate dialogue, but this cleaning process wasn’t particularly consistent as some scripts included scene descriptions, camera instructions such as “CUT TO” or “FADE TO”, blocking, or non-spoken character actions. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "This larger data set was then narrowed down by hand in order to create the ideal genre of movies to analyze. When deciding that we wanted to look at the behavior and speech patterns of villains in comparison to other characters and to themselves over time we decided to comb through the larger data set to find archetypical mystery movies to include in our data set.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Additionally, another data set we made use of was the NRC Word-Emotion Association Lexicon (Crowdsourcing a Word-Emotion Association Lexicon, Saif Mohammad and Peter Turney, Computational Intelligence, 29 (3), 436-465, 2013.) This dataset associates words with a binary score of 1 or 0 if they are correlated with any of the listed emotions or sentiments (anger, anticipation, fear, disgust, joy, negativity, positivity, sadness, surprise, trust). This dataset was compiled through crowdsourcing, where researchers asked participants on Amazon Mechanical Turk whether they thought a given word should be linked to these emotions."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### What processes might have influenced what data was observed and recorded and what was not?\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Some things that may have influenced the data could be the process by which we selected the movies and the characters. While neither were chosen arbitrarily, but based on a set of criteria, these decisions still had the potential to affect our larger analysis. For example, we chose movies based on a common set of criteria, thereby attempting to avoid any and all influence on our data in that regard, but it is still possible we added a movie falsely or skipped a movie we should have included. Alternatively, although we included what we viewed to be the most prominent characters, that may not hold true in the eyes of others, and could ultimately hurt our analyses in that regard as well. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Additionally, our total list of movies only includes scripts publicly available on the internet movie scripts database. There could be a selection bias here where only scripts that were easily accessible in the public domain or movies that were highly popular had their scripts added to the database. We viewed this as an acceptable risk given that high performing movies were likely to either trendsetters in the mystery movie genre (thus causing other movies to emulate them) or at the very least following the lead of other successful movies. Therefore, we view these scripts as descriptive of the mystery movie genre at large, even if they’re not necessarily a near-exhaustive sample. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### What preprocessing was done, and how did the data come to be in the form that you are using? What are the observations (rows) and the attributes (columns)?\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Our first intervention with the data involved isolating which movies should actually be considered mystery movies from a form standpoint. For this consideration we isolated three main components of the classic mystery movie that must be true in order for a movie to be included:\n",
    "\n",
    "- Some type of crime has been committed, and the main driver of the plot is trying to figure out who committed it.\n",
    "- The movie ends with the perpetrator or perpetrators of the crime identified, whether captured or not\n",
    "- The perpetrator or perpetrators were known in some other capacity before they were identified as the perpetrator, whether they were simply a suspect, a trusted friend, or an otherwise ordinary side character\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "This resulted in 55 movies being selected from the overall dataset of 862 films. Once we'd identified which movies fit our study, we created a metadata dictionary for each film with the following information:\n",
    "\n",
    "- the movie's filename (string)\n",
    "- the movie's title (string)\n",
    "- the year the movie came out (int)\n",
    "- the names of the four or five most important characters in the movie (Array of strings). While the defintion of who the most characters are is by definition arbitary, we measured it as an amalgamation of multiple factors, including the number of mentions they got in plot summaries of the movie and the number of lines they had. \n",
    "- the names of the villains (Array of strings). These were the names of the character or characters in the movie who were deemed most responsible for the crime being investigated. These names could either be members of the previous character list if they were important throughout the movie, or not if they were really only a side character before their reveal."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Once this metadata dictionary was hand compiled, we created a program to read through each script and separate dialogue based on the character who said them. We did this by isolating which lines are written in the style of a character tag (less than three spaces and all caps) and was not in a list of forbidden entries (these were mostly camera directions such as “CUT TO” “FADE TO” or narrative devices such as “VOICE OVER” or “THE END”). If the character was mentioned in either the character or villains list, we created an entry for that character with the following data points as columns:\n",
    "\n",
    "- character: String, the name of the character\n",
    "- title: String,  title of movie\n",
    "- year: Int, year of movie release\n",
    "- is_villain: Boolean, True if this character is in the villain list for their movie and False otherwise\n",
    "- raw_dialog: String, contains every word the character says stripped of extra spaces\n",
    "- num_words: Int, the total number of words the character says\n",
    "- token_list: Array of spacy token objects: the character’s dialogue split into tokens, with punctuation, numbers, and common uninformative or “stop” words (“is”, ”was”, “the”, etc.) removed\n",
    "- Mean_anger, mean_anticipation, mean_disgust, mean_fear, mean_joy, mean_positive, mean_negative, mean_sadness, mean_surprise, mean_trust: Float, 10 distinct variables representing how loaded a character’s dialogue was with each of the ten emotions on a scale of 0 to 1. This was calculated by adding together the scores for each word’s emotion as stated by our emotion lexicon and then dividing by the total number of words spoken.\n",
    "- Embeddings: Array of floats: a one dimensional array with 300 float values representing the average vector representation of a character’s dialogue in 300 dimensional space. This was calculated by taking the word2vec vector embedding of each word a character says and then finding the total mean for each dimension.  \n",
    "\n",
    "Each row in this instance is a different character in one of the movies\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "After the initial data was collected, we removed and corrected errors in the processing of the scripts. There were two main types of errors. The first was that the program would pick up scene descriptions or narration which used a characters name. We removed these from the dataset since they don't actually describe what the character says, merely how they're being shown by the camera. The second type of error was when the same character is referred to in multiple ways. One example was when lines spoken by the character Lee Blanchard in Black Dahlia were attributed to either \"Lee Blanchard\", \"Lee\", or \"Lee's voice\". We corrected these errors by amalgamating all lines into the most informative character name (in this case Lee Blanchard) and then removing the duplicated. All in all 51 such entries were removed combined according to these metrics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### If people are involved, were they aware of the data collection and if so, what purpose did they expect the data to be used for?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "While people were not really involved in the collection of this data as it was mainly a compilation of movies that are already existing publicly online, it did make use of the imsdb dataset, which was originally crowdsourced. However, contributors meant for their contributions to be used for such research purposes, and no personally identifiable information was gathered with this data\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Where can your raw source data be found, if applicable? Provide a link to the raw data (hosted in a Cornell Google Drive or Cornell Box). "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "- https://imsdb.com/ - This is the original database which held all of the original movie scripts.\n",
    "- https://nlds.soe.ucsc.edu/fc (Walker, Marilyn A., Ricky Grant, Jennifer Sawyer, Grace I. Lin, Noah Wardrip-Fruin, and Michael Buell. \"Perceived or Not Perceived: Film Character Models for Expressive NLG.\" BEST PAPER AWARD. In International Conference on Interactive Digital Storytelling (ICIDS), Vancouver, Canada, 2011) - This is the link to the collection of 862 scripts that were already chosen from the larger website, from which we collected our sampling. \n",
    "- https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm This is where we pulled the emotion lexicon from. \n",
    "- The file system where our emolex and applicable scripts can be found is: https://github.com/AsherML/info-2950/tree/main/data\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Data Limitations"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Our data set was a compilation of hand-selected movies that fit the archetypal mystery movie stereotype, with a villain to be found. We ended up with approximately 50 scripts to pull from, which means that the patterns we are seeing in villains only are based off of these 50 movies over time. This could be a representation concern if we were to be using this data to be representative of all films, but given that we are only discussing that one subset of mystery movies, This limiting aspect of our data should also be okay. The results we get could be vaguely generalizing for people who watch this particular sub genre of film, but if they are wary of our sample size then hopefully it should be alright. \n",
    "We are not analyzing the villains speech pattern off of every character in the movie, but off of five main characters who are not villains. This weakness was addressed while creating our data set and hopefully accounted for in final calculations. \n",
    "The data collected also is very variable depending on the film, in some films, the five main characters have a consistent number of lines and amount of interactions, but for others that is not the case. This skew could make it seem as though villains have a similar tone to regular characters in one movie, when in reality, the characters all have similar tones because they all live in the same town and have experienced similar things. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}