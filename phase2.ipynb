{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Phase 2\n",
    "\n",
    "## Data Collection and Cleaning\n",
    "\n",
    "Our raw text files existed scattered across a series of folders delineated by genre (examples included \"Crime\", \"Action\", \"Comedy\", etc.). In order to just work with the movies which met our \"classical mystery\" classification, we grouped all the movies we'd identified into a separate \"Using\" folder. At this point we had our movie scripts and our movie metadata files, which are stored in the drama_movies, crime_movies, and thriller_movies python files. These metadata files are an intermediate stage, but they're desribed in more detail in the dataset creation section. We then took the below steps in order to turn those text files into a workable dataset. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "First, we load the necessary libaries and metadata dictionaries, as well as create our spacy nlp object for later use"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "import os\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "#import the three libraries of assembled movie metdata\n",
    "from collections import defaultdict\n",
    "from drama_movies import drama_movies\n",
    "from crime_film_noir import crime_movies\n",
    "from thriller_movies import thriller_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "source": [
    "Then we created a set of convenience functions. This first one checks to see if a new line in a moviescript indicates a new character"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checks to see if a given line follows the same format as a new character tag\n",
    "def check_if_character(line, punct_set):\n",
    "    if \"(\" in line:\n",
    "        line=line[:line.index(\"(\")].strip()\n",
    "    if \"{\" in line:\n",
    "        line=line[:line.index(\"{\")].strip()\n",
    "    if '[' in line:\n",
    "        line=line[:line.index('[')].strip()\n",
    "    if \"/\" in line:\n",
    "            line=line[:line.index('/')-1].strip()\n",
    "    if line.upper()==line and line.isupper() and line[-1] not in punct_set and line.count(' ')<4:\n",
    "        \n",
    "        return True, line\n",
    "    else:\n",
    "        return False, \"\"\n"
   ]
  },
  {
   "source": [
    "This convenience function checks to see if the character that's currently being mentioned in the script is one of the characters we've previously identified as important, either for their own merit or for being the villain"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checks to see if this character is either one of the important characters in the story or a villain\n",
    "def check_if_acceptable_character(this_character, acceptable_characters):\n",
    "    if this_character in acceptable_characters:\n",
    "        return True\n",
    "    else:\n",
    "        cleaner=str.maketrans('','',string.punctuation)\n",
    "        this_character=this_character.translate(cleaner)\n",
    "        for character in acceptable_characters:\n",
    "            if character in this_character or this_character in character:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "        "
   ]
  },
  {
   "source": [
    "This is the beefiest convenience function, the one which actually reads the movie scripts. Instead of using this to directly create a pandas dataframe, we instead decided to make it a standard list in order to make it easier to iterate through each character and remove them. While the exact specifications are described in the method, essentially it creates a list of all the characters from across all movies and includes their name, the movie title, the year the movie's from, their dialogue, and the number of words they had to say in a dedicated dictionary. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"takes the movie metadata, reads each script, and returns a list made of dictionaries corresponding to each new character that's been labeled as important. The list and dictionaries follow the format below\n",
    "\n",
    "[{\"character\": \"the name of the character\", \"movie_title\": \"the title of the movie they're in\", \"year\" int(the year the movie came out), \"raw_dialogue\": \"All the words they say in the movie\", \"num_words\": int(the number of words they say in the movie)}]\n",
    "\"\"\"\n",
    "def read_scripts(movie_metadata, unacceptable_starters, punct_remover):\n",
    "\n",
    "    returner=[]\n",
    "\n",
    "    for movie_filename in movie_metadata:\n",
    "        path=os.path.join(\"data\", \"scripts\",\"Using\", movie_filename)\n",
    "        current_movie=movie_metadata[movie_filename]\n",
    "\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            movie_script=f.readlines()\n",
    "            start_of_content=False\n",
    "            this_movie_dicts=[]\n",
    "            current_character_dict={}\n",
    "            movie_title=movie_metadata[movie_filename]['title']\n",
    "            current_character_name=\"\" \n",
    "            year=int(current_movie['year'])\n",
    "            acceptable_characters=current_movie[\"characters\"]+current_movie['villain']      \n",
    "            \n",
    "            for line in movie_script:\n",
    "                line=line.strip()\n",
    "\n",
    "                #checks for the title and the first character, otherwise skips the header stuff\n",
    "                if not start_of_content:\n",
    "                    line=line.strip('\"')\n",
    "\n",
    "                    is_character, new_character_name = check_if_character(line, string.punctuation)\n",
    "\n",
    "                    if is_character and new_character_name not in unacceptable_starters and check_if_acceptable_character(new_character_name, acceptable_characters):\n",
    "                        start_of_content=True\n",
    "                        current_character_name=new_character_name\n",
    "                        current_character_dict={\"character\": current_character_name, \"movie_title\": movie_title, \"year\":year, \"is_villain\": False, \"raw_dialogue\":\"\"}\n",
    "                        if current_character_name in current_movie['villain']:\n",
    "                            current_character_dict['is_villain']=True\n",
    "                        this_movie_dicts.append(current_character_dict)\n",
    "                \n",
    "                #if in the middle of the script and you have a current character, check if this line is a new character otherwise add the dialogue to this character's list\n",
    "                else:\n",
    "                    is_new_character, new_character_name = check_if_character(line, string.punctuation)\n",
    "                    if is_new_character:\n",
    "                        current_character_name=new_character_name\n",
    "                        if current_character_name not in unacceptable_starters and check_if_acceptable_character(current_character_name, acceptable_characters):\n",
    "                            if current_character_name not in [entry['character'] for entry in this_movie_dicts]:\n",
    "                                current_character_dict={\"character\": current_character_name, \"movie_title\": movie_title, \"year\":year, \"is_villain\": False, \"raw_dialogue\":\"\"}\n",
    "                                if current_character_name in current_movie['villain']:\n",
    "                                    current_character_dict['is_villain']=True\n",
    "                                this_movie_dicts.append(current_character_dict)\n",
    "                            else:\n",
    "                                current_character_dict=[entry for entry in this_movie_dicts if entry['character']==current_character_name][0]\n",
    "                            \n",
    "                    else:\n",
    "                        if current_character_name in unacceptable_starters or not check_if_acceptable_character(current_character_name, acceptable_characters):\n",
    "                            pass\n",
    "                        else:\n",
    "                            if len(line)>0 and line[0]!='(' and line not in unacceptable_starters:\n",
    "                                current_character_dict['raw_dialogue']=current_character_dict['raw_dialogue'].strip()+' ' + line.translate(punct_remover).strip()\n",
    "\n",
    "            returner+=this_movie_dicts\n",
    "\n",
    "        \n",
    "    for entry in returner:\n",
    "        entry['num_words']=entry['raw_dialogue'].count(' ')+1\n",
    "    returner=[entry for entry in returner if entry['num_words']>50]    \n",
    "\n",
    "    return returner                            \n",
    "    "
   ]
  },
  {
   "source": [
    "While the above code works for most script formats, occassionally formatting errors in the script itself can result in errors. Additionally often the same character will be referred to by different names in a script. For example, in a single movie dialogue spoken by the character Harry Wilkens might be labelled as Harry, Mr. Wilkens, Harry's Voice, or any other permutations. Additionally, while the original researchers claimed to have removed setting description and camera directions from the scripts, they did not do so entirely. These often appear in the same format in the script as calls for new characters, causing countless errors. These two convenience functions below serve to elimniate these errors. The first deals with the issue of character permutations, allowing us to combine all the dialogue said by two characters and then deleting the extras from the list. The second is more simple, and is used to flat-out remove entries that were not characters but were added in error."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A convenience function that adds all lines from character_name_to_remove to character_name_to_keep and then removes character_name_to_remove from the list. \n",
    "def combine_characters(movie_character_dict, movie_title, character_name_to_keep, character_name_to_remove):\n",
    "    movie_subset=[entry for entry in movie_character_dict if entry['movie_title']==movie_title]\n",
    "    if character_name_to_remove not in [entry['character'] for entry in movie_subset] or character_name_to_keep not in [entry['character'] for entry in movie_subset]:\n",
    "        return\n",
    "    to_remove=[entry for entry in movie_character_dict if entry['character']==character_name_to_remove][0]\n",
    "    keeper=[entry for entry in movie_subset if entry['character']==character_name_to_keep][0]\n",
    "    keeper['raw_dialogue']+=to_remove['raw_dialogue']\n",
    "    keeper['num_words']+=to_remove['num_words']\n",
    "    movie_character_dict.remove(to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removes the character with this character name from the list\n",
    "def remove_character(movie_character_dicts, character):\n",
    "    to_remove=[entry for entry in movie_character_dicts if entry['character']==character]\n",
    "    if len(to_remove)!=0:\n",
    "        movie_character_dicts.remove(to_remove[0])"
   ]
  },
  {
   "source": [
    "The below function, given a pandas dataframe which includes a character's raw dialogue, will add a column including a list of all the tokens present in each character's dialogue that isn't punctuation, a space, or a commonly used filler or \"stop\" word"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#given a dataframe and a spacy nlp object, adds a column to the dataframe with an array of tokens present in each dialogue entry that aren't spaces, punctuation, or common stop words\n",
    "def tokenize(data_frame,nlp):\n",
    "    data_frame['token_list']=[[token for token in nlp(doc) if not (token.is_punct or token.is_space or token.is_stop)] for doc in data_frame['raw_dialogue']]"
   ]
  },
  {
   "source": [
    "The below function makes use of our second dataset, the NRC Word-Emotion Association Lexicon. Again, the specifics and sourcing of this lexicon are addressed more directly in our data collection section. However, in practice what this does is associate a set of given words with a set of ten sentiments: anger, anticipation, disgust, fear, joy negative, positive, sadness, surprise, trust. These associations are binary, if a word is associated with the sentiment it will have a score of 1, otherwise it will be 0. The below function runs through each of the tokens in a character's dialogue and calculates the average sentiment score for all of their words put togeher across the 10 sentiments."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#given a list of tokens, calculates a a mean emotional scoring of that list for each of the ten emotions present in the NRC Word-Emotion Association Emolex\n",
    "def add_emotion_scores(token_list, lexicon=None):\n",
    "    string_list=[token.text.lower().strip() for token in token_list]\n",
    "    re=[[],[],[],[],[],[],[],[],[],[]]\n",
    "    if lexicon==None:\n",
    "        lexicon=read_emolex()\n",
    "    for token in string_list:\n",
    "        if token in lexicon:\n",
    "            score_list=list(lexicon[token].values())\n",
    "            for score_index in range(len(score_list)):\n",
    "                re[score_index].append(score_list[score_index])\n",
    "    re=[score if len(score)>0 else [0] for score in re]\n",
    "    to_return = [np.mean(emotion) for emotion in re]\n",
    "    return to_return\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "source": [
    "The below function simply reads the emolex into a python dictionary, it was actually provided by Professor Wilkens for a previous class (INFO 3350)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: Professor Wilkens, INFO 3350\n",
    "#convenience function to read the emotion lexicon into the variable emolex\n",
    "def read_emolex(filepath=None):\n",
    "    '''\n",
    "    Takes a file path to the emolex lexicon file.\n",
    "    Returns a dictionary of emolex sentiment values.\n",
    "    '''\n",
    "    if filepath==None: # Try to find the emolex file\n",
    "        filepath = os.path.join('data','emolex.txt')\n",
    "        if os.path.isfile(filepath):\n",
    "            pass\n",
    "        elif os.path.isfile('emolex.txt'):\n",
    "            filepath = 'emolex.txt'\n",
    "        else:\n",
    "            raise FileNotFoundError('No EmoLex file found')\n",
    "    emolex = defaultdict(dict) # Like Counter(), defaultdict eases dictionary creation\n",
    "    with open(filepath, 'r') as f:\n",
    "    # emolex file format is: word emotion value\n",
    "        for line in f:\n",
    "            word, emotion, value = line.strip().split()\n",
    "            emolex[word][emotion] = int(value)\n",
    "    return emolex\n",
    "\n",
    "# Get EmoLex data. Make sure you set the right file path above.\n",
    "emolex = read_emolex()\n"
   ]
  },
  {
   "source": [
    "Another important factor we wanted to take into account was that while the words characters use might differ dramatically, the actual content of what they're trying to say might align very well. Word2Vec embeddings are essentially 300dimensional representations of a given word, with words of similar meanings being located near each other in this high dimensional space. Below, we calculate for each word a character says that word's embedding (or where it lies in this 300 dimensional space). We then average out these word embeddings (the mean of each dimension) to create a single 300 dimensional point that represents the character's dialogue as a whole. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#given a list of tokens, calculate the mean embedding for all token in the list\n",
    "def add_embeddings(token_list, vector_length):\n",
    "    token_list=[token for token in token_list if token.has_vector]\n",
    "    doc_matrix=np.zeros([len(token_list), vector_length])\n",
    "    for i in range(len(doc_matrix)):\n",
    "        doc_matrix[i]=token_list[i].vector\n",
    "    return np.average(doc_matrix, axis=0)"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Here we start actually assembling our dataset. We start by loading in our three separate metadata dictionaries into a single variable called amalgamated_dicts. we then declare a series of terms we cite as being unacceptable character names (often camera directions or non-character narration) and then store our list of character entities into movie_character_dicts"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "amalgamated_dicts={}\n",
    "amalgamated_dicts.update(drama_movies)\n",
    "amalgamated_dicts.update(crime_movies)\n",
    "amalgamated_dicts.update(thriller_movies)\n",
    "\n",
    "unacceptable_starters=[\"VOICE (cont'd)\", \"VOICE (CONT'D)\", \"VOICE OVER (CONT'D)\", \"VOICE OVER (cont'd)\", \"DISSOLVE\", \"CUT\", \"CUT TO\", 'FADE', 'FADE OUT', 'FADE IN', 'PAN', 'CONTINUED', \"CONT'D\", '', ' ', \"VOICE\", \"VOICE OVER\", 'CUT TO', 'DISSOLVE TO', 'THE END', 'FADE TO BLACK', \"DISSOLVE TO:\", \"CUT TO:\", \"FADE TO:\"]\n",
    "\n",
    "punct_remover=str.maketrans('','', '\"#$%&()*+-/:;<=>?@[\\\\]^_`{|}~')\n",
    "\n",
    "movie_character_dicts=read_scripts(amalgamated_dicts, unacceptable_starters, punct_remover)"
   ]
  },
  {
   "source": [
    "Below is a long list of miscalaneous fixes as described in the combine_characters and remove_character defintions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random fixes\n",
    "\n",
    "combine_characters(movie_character_dicts, \"8MM\", \"AMY\", \"AMY'S VOICE\")\n",
    "combine_characters(movie_character_dicts, \"8mm\", \"DINO VELVET\", \"DINO\")\n",
    "combine_characters(movie_character_dicts, \"8mm\", \"DINO VELVET\", \"DINO VELVET VOICE\")\n",
    "combine_characters(movie_character_dicts, \"8mm\", \"WELLES\", \"WELLES VOICE\")\n",
    "combine_characters(movie_character_dicts, \"8mm\", \"WELLES\", \"WELLES' VOICE\")\n",
    "combine_characters(movie_character_dicts, \"MANHATTAN MURDER MYSTERIES\", \"HELEN\", \"HELEN'S VOICE\")\n",
    "combine_characters(movie_character_dicts, 'The Black Dahlia', \"CAPTAIN VASQUEZ\", 'VASQUEZ')\n",
    "combine_characters(movie_character_dicts, 'The Black Dahlia', \"JOHNNY VOGEL\", 'JOHNNY')\n",
    "combine_characters(movie_character_dicts, 'The Black Dahlia', \"JOHNNY VOGEL\", 'VOGEL')\n",
    "combine_characters(movie_character_dicts, \"The Black Dahlia\", \"LEE BLANCHARD\", \"LEE\")\n",
    "combine_characters(movie_character_dicts, \"The Black Dahlia\", \"Liz\", \"Elizabeth\")\n",
    "combine_characters(movie_character_dicts, 'The Black Dahlia', \"ELLIS LOEW\", 'LOEW')\n",
    "combine_characters(movie_character_dicts, 'The Black Dahlia', \"RUSS MILLARD\", 'MILLARD')\n",
    "combine_characters(movie_character_dicts, 'BASIC INSTINCT', \"CAPTAIN TALCOTT\", 'TALCOTT')\n",
    "combine_characters(movie_character_dicts, 'BASIC INSTINCT', \"CAPTAIN TALCOTT\", 'CAPT. TALCOTT')\n",
    "combine_characters(movie_character_dicts, 'Basic', \"DUNBAR\", 'DUN BAR')\n",
    "combine_characters(movie_character_dicts, 'Basic', \"MUELLER\", 'MUE:LLER')\n",
    "combine_characters(movie_character_dicts, 'Basic', \"OSBORNE\", 'OSB0RNE')\n",
    "combine_characters(movie_character_dicts, 'The Girl With ', \"GREGOR\", 'GREGER')\n",
    "combine_characters(movie_character_dicts, 'THE GIRL WITH THE DRAGON TATTOO', \"GREGOR\", 'GREGER')\n",
    "combine_characters(movie_character_dicts, 'THE GIRL WITH THE DRAGON TATTOO', \"BLOMKVIST\", 'BLOMVIST')\n",
    "combine_characters(movie_character_dicts, 'THE GIRL WITH THE DRAGON TATTOO', \"HARRIET\", 'HARRIE')\n",
    "combine_characters(movie_character_dicts, 'THE GIRL WITH THE DRAGON TATTOO', \"WENNERSTROM\", 'WENNERSTROM ON TV')\n",
    "combine_characters(movie_character_dicts, 'THE GIRL WITH THE DRAGON TATTOO', \"VANGER\", 'YOUNGER VANGER')\n",
    "combine_characters(movie_character_dicts, 'Insomnia', 'WALTER', \"WALTER'S VOICE\")\n",
    "combine_characters(movie_character_dicts, \"Blood Simple\", \"MARTY\", \"MARTY'S VOICE\")\n",
    "combine_characters(movie_character_dicts, \"Bonnie and Clyde\", \"BONNIE\", \"BONNIE'S VOICE\")\n",
    "combine_characters(movie_character_dicts, \"Twin Peaks\", \"BOBBY\", \"BOB'S VOICE\")\n",
    "combine_characters(movie_character_dicts, \"Klute\", \"TRASK\", \"TRASK'S VOICE\")\n",
    "combine_characters(movie_character_dicts, \"Klute\", \"CABLE\", \"CABLE'S VOICE\")\n",
    "combine_characters(movie_character_dicts, \"Brick\", \"LAURA\", \"LAURA'S VOICE\")\n",
    "combine_characters(movie_character_dicts, \"Brick\", \"BRENDAN\", \"BRENDAN'S VOICE\")\n",
    "combine_characters(movie_character_dicts, \"Charade\", \"REGGIE\", \"REGGIE'S VOICE\")\n",
    "combine_characters(movie_character_dicts, \"Copycat\", \"DARYLL LEE\", \"DARYLL\")\n",
    "combine_characters(movie_character_dicts, \"Sherlock Holmes\", \"SHERLOCK\", \"HOLMES\")\n",
    "combine_characters(movie_character_dicts, \"Crank\", \"Verona\", \"Erona\")\n",
    "combine_characters(movie_character_dicts, \"Blood Simple\", \"DOC MILES\", \"OC MILES\")\n",
    "combine_characters(movie_character_dicts, \"Devil in a Blue Dress\", \"ALBRIGHT\", \"ALBRIGHT'S VOICE\")\n",
    "combine_characters(movie_character_dicts, \"Blood Simple\", \"DAPHNE\", \"DAPHNE'S VOICE\")\n",
    "combine_characters(movie_character_dicts, \"Anonymous\", \"ELIZABETH\", \"YOUNG ELIZABETH\")\n",
    "combine_characters(movie_character_dicts, \"Anonymous\", \"OXFORD\", \"YOUNG OXFORD\")\n",
    "combine_characters(movie_character_dicts, \"Anonymous\", \"ROBERT CECIL\", \"BOY ROBERT CECILDAPHNE'S VOICE\")\n",
    "\n",
    "remove_character(movie_character_dicts, \"BONNY & CLYDE\")\n",
    "remove_character(movie_character_dicts,\"I\")\n",
    "remove_character(movie_character_dicts, \"GRAMAM'S FEET\")\n",
    "remove_character(movie_character_dicts, \"HOLMES POV\")\n",
    "remove_character(movie_character_dicts, \"MRS. MULWRAY\")\n",
    "remove_character(movie_character_dicts,\"C\")\n",
    "remove_character(movie_character_dicts,\"S\")\n",
    "remove_character(movie_character_dicts,\"T\")\n",
    "remove_character(movie_character_dicts,\"H\")\n",
    "remove_character(movie_character_dicts,\"A\")\n",
    "remove_character(movie_character_dicts,\"I\")\n",
    "remove_character(movie_character_dicts,\"VE\")\n",
    "remove_character(movie_character_dicts,\"C\")\n",
    "remove_character(movie_character_dicts,\"DARKMAN\")\n",
    "remove_character(movie_character_dicts,\"161 PEYTON 161\")\n",
    "remove_character(movie_character_dicts,\"THE DARKMAN\")\n",
    "remove_character(movie_character_dicts,\"421 DARKMAN 421\")\n"
   ]
  },
  {
   "source": [
    "We finally create a dataframe from our list of dictionaries here, stored in the local_dataframe variable. We then added columns for word tokens, average emotional scores, and dialogue embeddings as described above"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_dataframe=pd.DataFrame.from_dict(movie_character_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenize(local_dataframe, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Wall time: 35.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "all_emotions = [add_emotion_scores(entry) for entry in local_dataframe['token_list']]\n",
    "local_dataframe['mean_anger'], local_dataframe['mean_anticipation'], local_dataframe['mean_disgust'], local_dataframe['mean_fear'], local_dataframe['mean_joy'], local_dataframe['mean_negative'], local_dataframe['mean_positive'], local_dataframe['mean_sadness'], local_dataframe['mean_surprise'], local_dataframe['mean_trust'] = np.flipud(np.rot90(all_emotions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Wall time: 1.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vector_length=nlp.vocab.vectors_length\n",
    "local_dataframe['embeddings']=[add_embeddings(entry, vector_length) for entry in local_dataframe['token_list']]"
   ]
  },
  {
   "source": [
    "Finally, the local_dataframe dataframe is written to the mystery_movie_data csv file in the same directory, ignoring the index"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_dataframe.to_csv(\"mystery_movie_data.csv\", index=False)"
   ]
  }
 ]
}